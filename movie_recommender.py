# -*- coding: utf-8 -*-
"""Movie Recommender.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dVYpX3B5NSlwAJ4uvjwSmTicN0H6Wv_1

# **Laporan Proyek Machine Learning - Gabril Hozanna**

## **Project Overview**

Pesatnya pertumbuhan pengumpulan data telah menyebabkan era baru informasi. Data digunakan untuk membuat sistem yang lebih efisien dan di sinilah Sistem Rekomendasi berperan. Sistem Rekomendasi adalah jenis sistem penyaringan informasi karena sistem Rekomendasi meningkatkan kualitas hasil pencarian dan menyediakan item yang lebih relevan dengan item pencarian atau terkait dengan riwayat pencarian penggunaan.

Sistem Rekomendasi digunakan untuk memprediksi peringkat atau preferensi yang akan diberikan pengguna pada suatu item. Hampir setiap perusahaan teknologi besar telah menerapkannya dalam beberapa bentuk atau yang lain: Amazon menggunakannya untuk menyarankan produk kepada pelanggan, YouTube menggunakannya untuk memutuskan video mana yang akan diputar selanjutnya di autoplay, dan Facebook menggunakannya untuk merekomendasikan halaman yang disukai dan orang untuk diikuti. Selain itu, perusahaan seperti Netflix dan Spotify sangat bergantung pada efektivitas mesin rekomendasi mereka untuk bisnis dan kesuksesan mereka. Di Netflix sendiri, menerapkan sistem rekomendasi untuk merekomendasikan film dan serial ke pengguna. Pendapatan Netflix pada kuartal II-2021 ini mengalami kenaikan 19 persen dari tahun ke tahun (YoY). Pendapatan perusahaan di tahun 2020 berada di angka `US$6,1` milliar (sekitar Rp.88,5 trilliun), naik menjadi `US$7,3` miliar (kira-kira Rp.105,9 trilliun) [InfoKomputer](https://infokomputer.grid.id/read/122837009/contoh-implementasi-teknologi-ai-di-platform-streaming-film-netflix?page=all).

Film adalah salah satu media hiburan yang populer di masyarakat. Sampai September 2021, tercatat telah ada 8,313,921 judul film yang telah rilis [IMDb Statistics](https://www.imdb.com/pressroom/stats/). Banyaknya judul-judul yang telah rilis membuat masyarakat kesulitan untuk menemukan film mana yang mereka ingin tonton. Untuk mengatasi masalah tersebut, perlu adanya informasi mengenai film yang akan memudahkan masyarakat untuk menemukan film yang cocok dengan preferensi user, oleh sebab itu user perlu sebuah sistem yang dapat memberikan rekomendasi film.

## **Business Understanding**

### **Problem Statements**

* Bagaimana menyajikan sejumlah rekomendasi film dengan teknik content-based filtering?
* Bagaimana menyajikan sejumlah rekomendasi film dengan teknik collaborative filtering?

### **Goals**
* Untuk menyajikan sejumlah rekomendasi film dengan teknik content-based filtering
* Untuk menyajikan sejumlah rekomendasi film dengan teknik collaborative filtering

### **Solution approach**

Berdasarkan problem statements yang telah disebutkan maka dapat menggunakan teknik:
1. **Content-based filtering** memanfaatkan informasi beberapa item / data untuk direkomendasikan kepada pengguna sebagai referensi yang terkait dengan informasi yang digunakan sebelumnya. Tujuan dari content based recommendation agar dapat memprediksi persamaan dari sejumlah informasi yang didapat dari pengguna. Content based filtering menggunakan konsep perhitungan vector, TF-IDF, dan Cosine Similarity yang intinya dikonversikan dari data / text menjadi berbentuk vektor. Content based filtering membutuhkan deskripsi item/data yang baik. 

  Untuk teknik content-based filtering menggunakan `CountVectorizer`. CountVectorizer akan mengambil kata-kata dari setiap kalimat dan menciptakan vocabulary dari semua kata unik dalam kalimat. Vocabulary ini kemudian dapat digunakan untuk membuat feature vector dari jumlah kata. Perbedaan antara `TfidfVectorizer()` dengan `CountVectorizer()` adalah `TfidfVectorizer()` mengembalikan float atau `returns floats` sedangkan CountVectorizer() mengembalikan int atau `return int`.

2. **Collaborative filtering** memanfaatkan transaksi suatu produk / item yang didasarkan kepada perilaku / kebiasaan si pengguna. Tujuannya agar pengguna yang sama dan item yang serupa dapat disukai oleh pengguna sebagai rekomendasi pilihan. Collaborative Filtering membutuhkan banyak feedback dari pengguna agar sistem berfungsi dengan baik.

  Untuk teknik collaborative filtering menerapkan model `BaseLine`, `SVD (Matrix Factorization)` dan `KNN` dari library `Surpise`. Library surpise sendiri digunakan untuk sistem rekomendasi. 
  * **Baseline Model**: Ini adalah algoritma dasar (basic) yang tidak banyak bekerja tetapi masih berguna untuk membandingkan akurasi. Digunakan untuk memprediksi perkiraan dasar untuk pengguna dan item tertentu.
  * **SVD Model**: Seperti yang dipopulerkan oleh [Simon Funk](https://sifter.org/~simon/journal/20061211.html) (Netflix Prize). Algoritma ini setara dengan Probabilistic Matrix Factorization [Sumber](https://surprise.readthedocs.io/en/stable/matrix_factorization.html#matrix-factorization-based-algorithms). Singular value decomposition (SVD) adalah metode faktorisasi matriks yang menggeneralisasikan dekomposisi eigen ([eigendecomposition atau eigenvectors/eigenvalues](https://medium.com/swlh/eigenvalues-and-eigenvectors-5fbc8b037eed)) dari matriks persegi (n x n) ke matriks apa pun (n x m) [Sumber](https://en.wikipedia.org/wiki/Singular_value_decomposition). 
  SVD mirip dengan Principal Component Analysis (PCA) tetapi lebih umum. PCA mengasumsikan bahwa inputannya berupa matriks persegi, SVD tidak memiliki asumsi ini. Secara umum rumusnya adalah `M=UŒ£V·µó`. SVD dapat menangani matriks dengan jumlah kolom dan baris yang berbeda. Rumus PCA adalah `M=ùëÑùö≤ùëÑ·µó` dimana menguraikan matriks menjadi matriks ortogonal ùëÑ dan matriks diagonal ùö≤. SVD melakukan hal serupa tetapi tidak kembali (return) ke basis yang sama saat memulai transformasi. Tidak bisa dilakukan karena matriks asli M bukan matriks persegi [Sumber](https://towardsdatascience.com/simple-svd-algorithms-13291ad2eef2).
  * **KNN Model**: Algoritma k-Nearest Neighbor adalah algoritma supervised
learning dimana hasil dari instance yang baru diklasifikasikan berdasarkan mayoritas dari kategori k-tetangga terdekat. Tujuannya adalah untuk mengklasifikasikan obyek baru berdasarkan atribut dan sample-sample dari training data. Algoritma k-Nearest Neighbor menggunakan Neighborhood Classification sebagai nilai prediksi dari nilai instance yang baru.
    * Kelebihan:
      1. Sangat nonlinear, bersifat nonparametrik dimana didefinisikan sebagai model nonparametrik karena model yang tidak mengasumsikan apa-apa mengenai distribusi instance di dalam dataset.
      2. Mudah dipahami dan diimplementasikan.
    * Kekurangan:
      1. Perlu menunjukkan parameter K (jumlah tetangga terdekat).
      2. Tidak menangani nilai hilang (missing value) secara implisit.
      3. Sensitif terhadap data pencilan (outlier).
      4. Rentan terhadap variabel yang non-informatif.
      5. Rentan terhadap dimensionalitas yang tinggi.
      6. Rentan terhadap perbedaan rentang variabel.
      7. Nilai komputasi yang tinggi. [Sumber](https://informatikalogi.com/algoritma-k-nn-k-nearest-neighbor/#3)

## **Data Understanding**

![Dataset](https://raw.githubusercontent.com/gabrielhozana/movie_recommender/main/photo/dataset.png)
**Konteks**

Dataset [The Movies Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset) berisi metadata untuk 45.000 film yang terdaftar di Dataset Full MovieLens. Kumpulan data terdiri dari film yang dirilis pada atau sebelum Juli 2017. Dataset berisi cast, crew, plot keywords, budget, revenue, posters, release dates, languages, production companies, countries, penghitungan suara TMDB, dan rata-rata vote.

Dataset ini juga memiliki file yang berisi 26 juta ratings dari 270.000 pengguna untuk 45.000 film. Ratings berada pada skala 1-5 dan telah diperoleh dari situs web resmi GroupLens.

<br>

**Konten**

Dataset ini terdiri dari file-file berikut:
* movies_metadata.csv: File Metadata Film utama. Berisi informasi tentang 45.000 film dari dataset Full MovieLens. Fitur terdiri dari posters, backdrops, budget, revenue, release dates, languages, production countries dan companies.

* keywords.csv: Berisi keyword plot film untuk film MovieLens. Tersedia dalam bentuk Objek JSON.

* credits.csv: Terdiri dari Informasi cast dan crew untuk semua film. Tersedia dalam bentuk Objek JSON.

* links.csv: File yang berisi ID TMDB dan IMDB dari semua film dari dataset Full MovieLens.

* links_small.csv: Berisi ID TMDB dan IMDB dari subset kecil dari 9.000 film dari dataset Lengkap.

* ratings_small.csv: Subset dari 100,000 ratings dari 700 pengguna di 9,000 film.


**Dataset**

Untuk dataset yang digunakan adalah `movies_metadata, keywords, credits, links_small dan ratings_small`. Dimana masing-masing dataset terdapat kolom/fitur yang berbeda. Untuk lebih jelasnya dapat dilihat dibawah ini:

1. **movies_metadata**

  Terdiri dari 45466 data namun masih terdapat banyak sekali data yang missing value (null/Nan) seperti pada kolom `overview, tagline, title`. Berikut ini adalah uraian beberapa kolom pada dataset yang akan digunakan:
  * **id**: merupakan id pada film.
  * **title**: merupakan judul pada film.
  * **original_title**: merupakan judul asli pada film
  * **tagline**: merupakan slogan atau catchphrases untuk film. Tagline dapat merujuk pada plot film.
  * **overview**: merupakan gambaran singkat terkait film.
  
2. **credits**
  
  Terdiri dari 45476 data dan tidak terdapat missing value (null/nan). Berikut ini adalah uraian kolom pada dataset.
  * **cast**: berisi terkait pemeran pada film.
  * **crew**: berisi terkait kru pada film.
  * **id**: merupakan id pada film.

3. **keywords**
  
  Terdiri dari 46419 data dan tidak terdapat missing value (null/nan). Berikut ini adalah uraian kolom pada dataset.
  * **id**: merupakan id pada film.
  * **keywords**: berisi berisi kata kunci atau keyword pada film.

4. **links_small**
  
  Terdiri dari 9125 data dan terdapat missing value pada kolom `tmdId`. Berikut ini adalah uraian kolom pada dataset.
  * **movieId**: merupakan id pada film.
  * **imdbId**: merupakan id pada database IMDB.
  * **tmdbId**: merupakan id pada database TMDB.
5. **ratings_small**
  
  Terdiri dari 100004 data dan tidak terdapat missing value (null/nan). Berikut ini adalah uraian kolom pada dataset. 
  * **userId**: merupakan id pengguna.
  * **movieId**: merupakan id pada film.
  * **rating**: merupakan rating yang diberikan oleh user terhadap film.
  * **timestamp**: berisi timestamp pada film.
  
  Pada kolom/fitur rating dapat dilihat pada gambar dibawah ini, dimana merupakan visualisasi dari distribusi rating film yang diberikan oleh pengguna. Jika dilihat, nilai rating film terbanyak adalah rating 4.   
  ![Rating](https://raw.githubusercontent.com/gabrielhozana/movie_recommender/main/photo/1.png)

**Exploratory Data Analysis (EDA)**
Dilakukan Exploratory Data Analysis (EDA) untuk mendapatkan sebuah insight dari dataset. Adapun tahapan yang dilakukan adalah:
* Melakukan analisis deskriptif `describe`. 
* Mengecek informasi data `info` dan missing value.
* Melakukan visualisasi data pada dataset `ratings`.

### **Exploratory Data Analysis (EDA)**

**Load Dataset**

Menginstall library kaggle dan mengupload api kaggle untuk dapat mengunduh langsung dataset dari kaggle.
"""

!pip install kaggle

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
  
# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

"""Mengunduh dan mengekstrak dataset movie.

Dengan perintah `%%bash` mengakses langsung dari kernel linux dalam hal ini tanpa perlu menggunakan perintah `!` diawal perintah (command) untuk mengeksekusi sebuah perintah linux.

Untuk dapat mengunduh dataset dari kaggle maka dapat menggunakan perintah (command) `kaggle datasets download` dan disertakan dengan `<user kaggle>/<nama_dataset>`

Terakhir mengekstrak dataset dengan perintah `unzip <nama_file>`
"""

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# kaggle datasets download -d rounakbanik/the-movies-dataset
# 
# unzip the-movies-dataset.zip

"""Load library yang diperlukan dalam pengembangan model dan analisa data."""

import pandas as pd
!pip install surprise
from surprise import Reader, Dataset, BaselineOnly,KNNBasic, SVD, accuracy
from surprise.model_selection import train_test_split

movies = pd.read_csv('movies_metadata.csv')
ratings = pd.read_csv('ratings_small.csv')
credits = pd.read_csv('credits.csv')
keywords = pd.read_csv('keywords.csv')
links = pd.read_csv('links_small.csv')

"""**Check Dataset**"""

print('Movie',movies.shape)
print('Credits',credits.shape)
print('Keywords',keywords.shape)
print('Links',links.shape)
print('Ratings',ratings.shape)

"""**Movies Metadata**"""

movies.info()

"""Terdapat banyak data missing value hampir disetiap kolom."""

movies.head()

"""Ada beberapa kolom yang dapat digunakan untuk membuat sistem rekomendasi, namun kita akan mempertimbangkan terkait kolom/fitur title, overview dan tagline"""

movies[['id','title','overview','tagline']].isnull().sum()

print('Jumlah ID:', len(movies.id.unique()))
print('Jumlah Title:', len(movies.title.unique()))
print('Jumlah Overview:', len(movies.overview.unique()))
print('Jumlah Tagline:', len(movies.tagline.unique()))

"""**Credits**"""

credits.info()

"""Tidak terdapat missing value pada dataset credits."""

credits.head()

"""Jika dilihat, datanya berupa data object json."""

credits.isnull().sum()

"""**Keywords**"""

keywords.info()

"""Tidak terdapat missing value."""

keywords.head()

"""Data berupa data object json."""

keywords.isnull().sum()

"""**Links Small**"""

links.info()

"""Terdapat missing value pada kolom `tmdbId`."""

links.describe()

links.head()

links.isnull().sum()

"""**Ratings Small**"""

ratings.info()

ratings.describe()

ratings.head()

print('Rating Terendah:', min(ratings['rating']))
print('Rating Tertinggi:', max(ratings['rating']))
print('Rating Keseluruhan:', sorted(ratings.rating.unique()))

"""Dapat dilihat bahwa rating yang diberikan berada pada nilai 0.5 - 5.0"""

print(ratings['rating'].value_counts())
ratings['rating'].value_counts().plot(kind='bar', title='rating');

"""Pemberian nilai rating tertinggi pada film adalah rating dengan nilai 4.

## **Data Preparation**

**Ubah Tipe Data Movies**

Diketahui bahwa pada dataset movie tepatnya kolom/fitur `id` tipe datanya berupa `object`. Perlu dikonversi ke bentuk `int`.

Sebelum mengubah fitur `id` ke tipe `int`, terdapat data anomali. Jika langsung mengubahnya ke tipe `int` maka akan menghasilkan error oleh sebab itu harus ditangani data anomali tersebut. Untuk itu dilakukan pengecekan data anomali.
"""

movies.loc[movies['id']=='1997-08-20']

movies.loc[movies['id'].str.contains('-')]

movies = movies.drop([19730, 29503, 35587])

import numpy as np

print(movies['id'].dtype)
movies['id'] = movies['id'].astype(int)
print(movies['id'].dtype)

"""**Ubah Tipe Data Links Small**

Seperti yang diketahui terdapat 13 data yang null dan jika diperhatikan pada analisis deskriptif, kolom/fitur pada `tmdbId` nilainya bukan desimal namun tipe datanya float. Maka bisa diubah ke tipe data `int`.
"""

links = links[links['tmdbId'].notnull()]['tmdbId'].astype('int')

links.head()

"""**Menyamakan Dataframe dengan Values**"""

smd = movies[movies['id'].isin(links)]
smd.shape

"""Terdapat 9099 film yang tersedia dalam dataset metadata movies yang 5 kali lebih kecil dari dataset asli yang terdiri dari 45000 film. Hanya mengambil sebagian data dari kesamaan dataset Metadata dengan Links Small.

**Mengisi Nilai Null/Nan**
"""

smd['tagline'].unique()

"""Terdapat data nan pada kolom/fitur tagline. Untuk mengatasi hal tersebut maka data nan tersebut diisi dengan `''`.

**Menggabungkan Kolom/Fitur**

Tagline film adalah slogan atau catchphrases untuk film. Biasanya menyertakan permainan kata-kata yang cerdas, frasa pendek, satu atau dua kalimat. Tagline dapat merujuk pada plot film atau menyarankan pengalaman yang akan dialami sebagai penonton. Jadi, akan menggabungkan kolom/fitur `tagline` dengan kolom/fitur `overview` untuk memperoleh kolom/fitur `description`.
"""

smd['tagline'] = smd['tagline'].fillna('')
smd['description'] = smd['overview'] + smd['tagline']
smd['description'] = smd['description'].fillna('')

"""**Dataset Credits & Keywords**

**Menggabungkan Dataset**
"""

keywords['id'] = keywords['id'].astype('int')
credits['id'] = credits['id'].astype('int')
movies['id'] = movies['id'].astype('int')

"""Akan menggunakan crew, cast dan keywords, jadi mari kita gabungkan dataframe ini dengan dataframe metadata movies."""

movies2 = movies.merge(credits, on='id',how='left')

movies2 = movies2.merge(keywords, on='id',how='left')

"""Hanya akan menyimpan film yang ada di tmdb `id` dan akan membuat dataframe baru sehingga akan membiarkan dataframe asli tanpa perubahan (dataset metadata movies).

"""

smd2 = movies2[movies2['id'].isin(links)]
smd2.shape

"""Sekarang memiliki kolom/fitur cast, crew, genres and credits, semuanya dalam satu dataframe (Dataset Metadata Movie). Sehingga dapat menarik sebuah intuisi:

* **Crew**: Dari kru, hanya akan memilih/mengambil sutradara (director) sebagai fitur karena yang lain tidak banyak berkontribusi pada nuansa film.
* **Cast**: Memilih pemeran sedikit lebih rumit. Aktor yang kurang dikenal dan peran kecil tidak terlalu memengaruhi opini orang tentang sebuah film. Oleh karena itu hanya memilih karakter utama dan aktornya masing-masing. Sewenang-wenang akan memilih 3 aktor teratas yang muncul dalam daftar credits.

"""

smd2.info()

"""Akan mengubah kolom/fitur `cast, crew, keywords` menjadi hanya berisi satu list dengan `cast, crew, keywords` bukan dictionary.

"""

from ast import literal_eval
smd2['cast'] = smd2['cast'].apply(literal_eval)
smd2['crew'] = smd2['crew'].apply(literal_eval)
smd2['keywords'] = smd2['keywords'].apply(literal_eval)
smd2['cast_size'] = smd2['cast'].apply(lambda x: len(x))
smd2['crew_size'] = smd2['crew'].apply(lambda x: len(x))

"""**Get Director**

Kru film terdiri dari puluhan orang dan peran, mulai dari tugas yang sederhana hingga yang paling rumit. Janya akan memilih sutradara karena peran lain tidak banyak berkontribusi pada nuansa film.
"""

#membuat fungsi mendapatkan data sutradara
def get_director(x):
    for i in x:
        if i['job'] == 'Director':
            return i['name']
    return np.nan

smd2['director'] = smd2['crew'].apply(get_director)

smd2.director.head()

"""Setiap film memiliki banyak aktor, tidak semuanya begitu penting, jadi hanya akan memilih tiga aktor teratas dalam daftar pemeran.

"""

smd2['cast'] = smd2['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])
smd2['cast'] = smd2['cast'].apply(lambda x: x[:3] if len(x) >=3 else x)

smd2['keywords'] = smd2['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])

smd2[['director','cast','keywords']].head()

"""**Hapus Spasi dan Ubah ke Lowercase**

Dengan cara ini, mesin tidak akan bingung misalnya membedakan Johnny Depp dan Johnny Galecki.
"""

smd2['cast'] = smd2['cast'].apply(lambda x: [str.lower(i.replace(" ", "")) for i in x])

"""**Mention Director 3x**

Mention Sutradara 3 kali untuk memberikan bobot yang lebih besar dibandingkan dengan seluruh pemeran karena sutradara mempengaruhi kualitas film lebih dari peran lainnya.
"""

smd2['director'] = smd2['director'].astype('str').apply(lambda x: str.lower(x.replace(" ", "")))
smd2['director'] = smd2['director'].apply(lambda x: [x,x, x])

smd2[['cast','director']].head()

"""**Keywords**

Menghitung jumlah frekuensi dari setiap kata kunci yang muncul di dataset.
"""

s = smd2.apply(lambda x: pd.Series(x['keywords']),axis=1).stack().reset_index(level=1, drop=True)
s.name = 'keyword'

s = s.value_counts()
s[:5]

"""Keywords muncul dalam frekuensi mulai dari 1 hingga 610. Jadi tidak akan menggunakan keywords yang hanya muncul sekali. Oleh karena itu, ini dapat dihapus dengan aman. Terakhir, akan mengonversi setiap kata dengan teknik `stemming` sehingga kata-kata seperti Dog dan Dogs dianggap sama.

Hanya akan mengambil film dengan lebih dari satu keywords.
"""

s = s[s > 1]

from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import wordnet

stemmer = SnowballStemmer('english')
stemmer.stem('dogs')

def filter_keywords(x):
    words = []
    for i in x:
        if i in s:
            words.append(i)
    return words

smd2['keywords'] = smd2['keywords'].apply(filter_keywords)
smd2['keywords'] = smd2['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])
smd2['keywords'] = smd2['keywords'].apply(lambda x: [str.lower(i.replace(" ", "")) for i in x])

smd2['genres'] = smd2['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])

"""Sekarang akan membuat kolom/fitur secara umum yang berisi semua data yang telah dilakukan pada tahap data preparation seperti keywords, cast, director dan genres."""

smd2['overall'] = smd2['keywords'] + smd2['cast'] + smd2['director'] + smd2['genres']
smd2['overall'] = smd2['overall'].apply(lambda x: ' '.join(x))

smd2.overall.head(2)

"""**Ratings**

Melakukan drop pada kolom/fitur `timestamp`
"""

ratings.drop("timestamp", axis = 1, inplace = True)

"""**Split Ratings**"""

reader = Reader()
surprise_data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)
trainset, testset = train_test_split(surprise_data, test_size=0.25)

"""Terdapat 2794 film dan 671 pengguna.

## **Modeling**

###Model Development dengan Content Based Filtering

Sebelumnya telah melakukan data preparation, dimana telah mendapatkan dua dataframe baru. Kedua dataframe yang didapatkan akan dibuat model berdasarkan kolom/fitur yang berbeda. Adapun dataframe yang didapatkan beserta kolom/fitur yang akan digunakan:

* `smd`: Movie Overviews dan Taglines.
* `smd2`: Movie Cast, Crew, Keywords dan Genre.

**Movie Overviews dan Taglines**

Di definisikan variabel vectorizer sebagai CountVectorizer(), dengan stop_words=‚Äôenglish‚Äô berfungsi untuk menghilangkan kata semacam: i, you, the, a, this, is dan sejenisnya.
"""

from sklearn.feature_extraction.text import CountVectorizer

count = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')
count_matrix = count.fit_transform(smd['description'])

count_matrix.shape

"""Jika diperhatikan, matriks yang didapatkan berukuran (9099, 268124). Nilai 9099 merupakan ukuran data dan 268124 merupakan matrik kategori description.

**Movie Cast, Crew, Keywords dan Genre**
"""

count2 = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')
count_matrix2 = count2.fit_transform(smd2['overall'])

count_matrix2.shape

"""Jika diperhatikan, matriks yang didapatkan berukuran (9219, 107377). Nilai 9219 merupakan ukuran data dan 107377 merupakan matrik kategori overall.

###Model Development dengan Collaborative Filtering

Dibuat sebuah fungsi evaluasi model untuk menghasilkan output berupa metric MSE dan RMSE. Fungsi evaluasi model ini akan digunakan dalam mengevaluasi beberapa model yang akan digunakan.
"""

def evaluate_model(train,test, print_metric = True):
  rmse_train = accuracy.rmse(train, verbose = False)
  rmse_test = accuracy.rmse(test, verbose = False)
  mse_train = accuracy.mse(train,verbose = False)
  mse_test = accuracy.mse(test, verbose =False)
  if print_metric:
    print("RMSE Data Training: ", rmse_train)
    print("MSE Data Training: ", mse_train)
    print("RMSE Data Test: ", rmse_test)
    print("MSE Data Test: ", mse_test)
  return rmse_train, mse_train, rmse_test, mse_test

"""Pada tahap ini, akan dikembangkan model machine learning dengan tiga algoritma. Kemudian akan mengevaluasi performa masing-masing algoritma. Dalam kasus kali ini, model harus menggunakan algoritma sistem rekomendasi karena hasil prediksi yang diinginkan adalah sejumlah rekomendasi film.

Algoritma yang digunakan, yaitu:
* BaselineOnly
* SVD
* KNN

**BaselineOnly**
"""

baseline_model = BaselineOnly(verbose = False)
baseline_model.fit(trainset)

train_predictions = baseline_model.test(trainset.build_testset())
test_predictions = baseline_model.test(testset)
_ ,_ ,rmse, mse = evaluate_model(train_predictions,test_predictions)

"""Kode dibawah digunakan untuk menyimpan evaluasi model data validasi (data uji) yang akan dibandingkan dengan model lainnya berdasarkan metrik MSE dan RMSE."""

results_df = pd.DataFrame([['BaselineOnly', rmse,mse]], columns=['Model', 'RMSE', 'MSE'])
results_df

"""**SVD**"""

svd = SVD(verbose = False) 
svd.fit(trainset)

train_predictions2 = svd.test(trainset.build_testset())
test_predictions2 = svd.test(testset)
_ ,_ ,rmse_svd, mse_svd = evaluate_model(train_predictions2,test_predictions2)

results_svd = pd.DataFrame([['SVD', rmse_svd, mse_svd]], columns=['Model', 'RMSE', 'MSE'])
results_df = results_df.append(results_svd, ignore_index=True)
results_df

"""**KNN**"""

knn= KNNBasic(k = 15, min_k= 5,random_state = 42)
knn.fit(trainset)

train_predictions3 = knn.test(trainset.build_testset())
test_predictions3 = knn.test(testset)
_ ,_ ,rmse_knn, mse_knn = evaluate_model(train_predictions3,test_predictions3)

results_knn = pd.DataFrame([['KNN', rmse_knn,mse_knn]], columns=['Model', 'RMSE', 'MSE'])
results_df = results_df.append(results_knn, ignore_index=True)
results_df

"""## **Evaluation**

### Evaluation Model dengan Content Based Filtering

**Movie Overviews dan Taglines**

Sekarang, akan menghitung derajat kesamaan (similarity degree) antar description film dengan teknik cosine similarity. Di sini, kita menggunakan fungsi cosine_similarity dari library sklearn.
"""

from sklearn.metrics.pairwise import linear_kernel, cosine_similarity
cosine_sim = cosine_similarity(count_matrix, count_matrix)
cosine_sim

"""Pada tahapan ini, menghitung cosine similarity dataframe count_matrix yang diperoleh pada tahapan sebelumnya. Dengan satu baris kode untuk memanggil fungsi cosine similarity dari library sklearn, telah berhasil menghitung kesamaan (similarity) antar `description` film. Kode di atas menghasilkan keluaran berupa matriks kesamaan dalam bentuk array. 

Selanjutnya, melihat matriks kesamaan setiap `description` film dengan menampilkan nama `description` film dalam 5 sampel kolom (axis = 1) dan 10 sampel baris (axis=0).
"""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa deskripsi film
cosine_sim_df = pd.DataFrame(cosine_sim, index=smd['description'], columns=smd['description'])
print('Shape:', cosine_sim_df.shape)
 
# Melihat similarity matrix pada setiap deskripsi film
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Dengan cosine similarity berhasil mengidentifikasi kesamaan antara satu description film dengan description film lainnya. Shape (9099, 9099) merupakan ukuran matriks similarity dari data yang dimiliki. Ukuran tersebut merupakan identifikasi tingkat kesamaan pada 9099 description film terhadap sumbu X dan sumbu Y. Namun hanya diambil sampel 10 description film pada baris vertikal dan 5 sampel description film pada sumbu horizontal. 

Jika diperhatikan hasilnya, terdapat angka-angka dimana jika angka tersebut mendekati 1 artinya semakin memiliki kesamaan dengan description film (sumbu x dengan sumbu y).

**Movie Cast, Crew, Keywords dan Genre**

Selanjutnya, akan menghitung derajat kesamaan (similarity degree) antar overall film dengan teknik cosine similarity. Di sini, kita menggunakan fungsi cosine_similarity dari library sklearn.
"""

cosine_sim2 = cosine_similarity(count_matrix2, count_matrix2)
cosine_sim2

"""Menghitung cosine similarity dataframe count_matrix2 yang diperoleh pada tahapan sebelumnya. Dengan satu baris kode untuk memanggil fungsi cosine similarity dari library sklearn, telah berhasil menghitung kesamaan (similarity) antar `overall` film. Kode di atas menghasilkan keluaran berupa matriks kesamaan dalam bentuk array. 

Selanjutnya, melihat matriks kesamaan setiap `overall` film dengan menampilkan nama `overall` film dalam 5 sampel kolom (axis = 1) dan 10 sampel baris (axis=0).
"""

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa keywords, cast, director dan genres film
cosine_sim_df2 = pd.DataFrame(cosine_sim2, index=smd2['overall'], columns=smd2['overall'])
print('Shape:', cosine_sim_df.shape)
 
# Melihat similarity matrix pada setiap keywords, cast, director dan genres film
cosine_sim_df2.sample(5, axis=1).sample(10, axis=0)

"""### Evaluation Model dengan Collaborative Filtering"""

import matplotlib.pyplot as plt
import seaborn as sns

sns.set(rc = {'figure.figsize':(10,6)})
sns.set_palette('Set2')
sns.set_style("whitegrid")
sns.barplot(x='RMSE', y='Model', data=results_df)
plt.title('RMSE for each model')
plt.show()

"""Nilai RMSE terendah adalah model baseline."""

sns.barplot(x='MSE', y='Model', data=results_df)
plt.title('MSE for each model')
plt.show()

"""Melakukan visualisasi MSE yang dimana hasil dari MSE  model baseline menghasilkan nilai yang kecil dibandingkan dengan kedua model lainnya.

## **Prediction**

###Prediksi dengan Content Based Filtering

**Movie Overviews dan Taglines**
"""

smd = smd.reset_index()
titles = smd['title']
indices = pd.Series(smd.index, index=smd['title'])

"""Membuat sebuah fungsi untuk mendapatkan sejumlah rekomendasi film. 
Dengan mengambil beberapa data yang mirip (similarity) dan akan mengambil data dari bobot (tingkat kesamaan) tertinggi ke terendah dan memasukkannya ke variabel `sim_scores`.
"""

def get_recommendations(title):
    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:31]
    movie_indices = [i[0] for i in sim_scores]
    sim_values = [i[1]for i in sim_scores]
    return titles.iloc[movie_indices], sim_values

"""Membuat sebuah fungsi evaluasi metrik untuk mengevaluasi seberapa presisi film yang disarankan. Metrik yang dibuat adalah metrik `precision`."""

def precision(gr,sv):
  result = []
  gr = len(gr)
  sv = sv[:gr]
  for x in sv:
    if x>0.1:
      result.append(x)
  results = len(result)
  precision = results/gr
  print("Jumlah Rekomendasi Film yang sesuai: ", len(result))
  print("Banyaknya Rekomendasi Film: ", gr)
  return round(precision,2)

"""Hasil prediksi"""

gr,sv = get_recommendations('The Dark Knight')
gr

"""Metrik Precision"""

precision(gr,sv)

"""**Movie Cast, Crew, Keywords dan Genre**"""

smd2 = smd2.reset_index()
titles2 = smd2['title']
indices = pd.Series(smd2.index, index=smd2['title'])

def get_recommendations2(title):
    idx = indices[title]
    sim_scores = list(enumerate(cosine_sim2[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    sim_scores = sim_scores[1:31]
    movie_indices = [i[0] for i in sim_scores]
    sim_values = [i[1]for i in sim_scores]
    return titles.iloc[movie_indices], sim_values

"""Hasil prediksi"""

gr2, sv2 = get_recommendations2('Mean Girls')
gr2.head(10)

"""Metrik Precision"""

precision(gr2.head(10),sv2)

"""###Prediksi dengan Collaborative Filtering"""

def get_top_n_recommendations(userId,predictions, n=10):
    predict_ratings = {}
    # loop untuk mendapatkan prediksi untuk user
    for uid, iid, true_r, est, _ in predictions:
        if (uid==userId):
            predict_ratings[iid] = est
    predict_ratings = sorted(predict_ratings.items(), key=lambda kv: kv[1],reverse=True)[:n]
    top_movies = [i[0] for i in predict_ratings]
    top_movies = [str(i) for i in top_movies]
    print("="*10,"Recommended movies for user {} :".format(userId),"="*10)
    print(movies2[movies2["id"].isin(top_movies)]["original_title"].to_string(index=False))

"""**Prediksi dengan model Baseline**"""

bm = get_top_n_recommendations(450,test_predictions)

"""**Prediksi dengan model SVD**"""

sm = get_top_n_recommendations(450,test_predictions2)

"""**Prediksi dengan model KNN**"""

km = get_top_n_recommendations(450,test_predictions3)

"""Hasil rekomendasi film dari ketiga model yang dikembangkan, menghasilkan sejumlah rekomendasi film yang sama. Jika diperhatikan metrik dari ketiga model, nilainya metriknya tidak jauh berbeda."""

